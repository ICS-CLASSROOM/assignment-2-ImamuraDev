{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27993d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0d3e6",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In this first part, we're going to use Spark to analyze the following books, which Iahev downloaded from Project Gutenberg and saved to the data folder.\n",
    "\n",
    "| File name | Book Title|\n",
    "|:---------:|:----------|\n",
    "|43.txt | The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson|\n",
    "|84.txt | Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley |\n",
    "|398.txt  | The First Book of Adam and Eve by Rutherford Hayes Platt|\n",
    "|3296.txt | The Confessions of St. Augustine by Bishop of Hippo Saint Augustine|\n",
    "\n",
    "Our objective is to cluster these 7 books based on thier similarity in terms of their most frequent context specific words, i.e. note \"the\", \"and\", \"or\", etc..\n",
    "\n",
    "* One we've generated those workds for these 7 works, we will use those vectors to generate a hierarchical clustering for that shows the simialrity between these books.\n",
    "\n",
    "For this assignment, you will need to make sure you're running from a PySpark docker environment I introduced in class. You can start the docker pySpark docker environment using the following command:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "Make sure you run the command from the directory containing this jupyter notebook and your data folder.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecd48e",
   "metadata": {},
   "source": [
    "### Prologue\n",
    "\n",
    "An important aspect of Natural Lanaguge Processing is the identification of texts that are similar. A naive approach to decide whether two documents are similar is by treating  a book as a collection of words (or, bag of words) and compare the documents based on these words. For example, one would expect two books the topic of which is religion  (ex. books 398.txt and  3296.txt), to have more words in common that words than a book that talks about religion and a book that discusess science fiction (ex books 84.txt and 398.txt). \n",
    "\n",
    "As mentioned above, we will be using Spark to analyze the data. While Spark is not necessary for such a small example, the plateform would be idea for analyzing a very large collection of documents, such those are often handled by large comapnies\n",
    "\n",
    "This part of the assignment will rely exclusively on RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15530",
   "metadata": {},
   "source": [
    "QX. We'll start by importing Spark and making sure our environemnt is set up properly for the assignment.\n",
    "\n",
    "Import the spark context necesarry to load a document as an RDD\n",
    "\n",
    "* Ignore  any error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80df4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133cbb2",
   "metadata": {},
   "source": [
    "QX Read in the file `43.txt` as a spark RDD and save it to the variable book_43\n",
    " * make sure book_43 of type MapPartitionsRDD\n",
    "   * str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101f3894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_43 = sc.textFile('data/43.txt')\n",
    "str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c01236",
   "metadata": {},
   "source": [
    "QX How many lines does `book_43` the file contain?\n",
    "* You can only use operaration or actions to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses function such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e5309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2935"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_43.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee25cc",
   "metadata": {},
   "source": [
    "QX We need to first remove the occurrences of non-alphabetical characters and numbers. You can use the following function, which given a line, remove digist and non-word characters and splits it into a collection of word \n",
    "\n",
    "```python\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "```\n",
    "\n",
    "Use the fucntion above on the variable (test_line) to see what it returns.\n",
    "```python\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98f09a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'and',\n",
       " 'a',\n",
       " 'dash',\n",
       " 'containing',\n",
       " 'number']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_split_line(line):\n",
    "    line = re.sub('\\d+', '', line)\n",
    "    line = re.sub('[\\W]+', ' ', line)\n",
    "    return line.split()\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\"\n",
    "clean_split_line(test_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64f2947",
   "metadata": {},
   "source": [
    "QX How many distinct words does this book contain.  To answer this question, you may find it useful to apply the function in a spark-fashion. Also, give\n",
    "\n",
    "* You can only use operaration or actions to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses function such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6b536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be73ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
